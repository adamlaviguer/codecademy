{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a633f7",
   "metadata": {},
   "source": [
    "# Python Data Analysis Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca3ecf",
   "metadata": {},
   "source": [
    "## Common Libraries to Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aed245",
   "metadata": {},
   "source": [
    "```py\n",
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import iqr, ttest_ind, pearsonr, trim_mean, chi2_contingency, ttest_1samp, binom_test\n",
    "\n",
    "# Machine Learning & Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d613fcf",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e848b2e6",
   "metadata": {},
   "source": [
    "```py\n",
    "df.head()               # First 5 rows\n",
    "df.info()               # Data types & non-null counts\n",
    "df.describe()           # Summary statistics\n",
    "df.columns              # Column names\n",
    "df.shape                # Rows and columns\n",
    "df.isnull().sum()       # Missing values\n",
    "df.column.value_counts()       # List the unique values in a column and the count of each value\n",
    "df.column.value_counts(normalize=True)  # List the unqiue values in a column and the proportion of each value\n",
    "pd.crosstab(df.column1, df.column2)     # Crosstab function from pandas to create a contingency table\n",
    "crosstab_object / len(df)               # Converting the crosstab object to proportional values\n",
    "crosstab_prop_object.sum(axis = 0)      # Get marginal proportion of the Y axis\n",
    "crosstab_prop_object.sum(axis = 1)      # Get marginal proprotion of the X axis\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f873358",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8da156",
   "metadata": {},
   "source": [
    "```py\n",
    "df.dropna()                         # Drop missing values\n",
    "df.fillna(value)                    # Fill missing values\n",
    "df.duplicated().sum()               # Count duplicates\n",
    "df.drop_duplicates(inplace=True)    # Remove duplicates\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094c91b",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0b837",
   "metadata": {},
   "source": [
    "### Line Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25b5d1",
   "metadata": {},
   "source": [
    "#### Using `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeee35b",
   "metadata": {},
   "source": [
    "```py\n",
    "# Single line plot\n",
    "plt.plot(x_axis_list, y_axis_list)\n",
    "plt.show()\n",
    "\n",
    "# Multi-line plot. Matplotlib will automatically place the two lines on the same axes and give them different colors if you call plt.plot() twice.\n",
    "plt.plot(x_axis_list, y_axis_list)\n",
    "plt.plot(x_axis_list, y_axis_list_2)\n",
    "plt.show()\n",
    "\n",
    "# Linestyles keywords\n",
    "color = 'green' or '#AAAAAA'      # We can specify a different color for a line by using the keyword color with either an HTML color name or a Hex code\n",
    "linestyle = '--' or ':' or ''       # Change the linestyle to dotted or dashed\n",
    "marker = 'o' or 's' or '*'          # Add a line marker like a dot or a square\n",
    "\n",
    "# Shaded error region\n",
    "lower_bound_y = [i - 2 for i in y_axis_list]    # represents an error of 2 for each y value\n",
    "upper_bound_y = [i + 2 for i in y_axis_list]\n",
    "plt.fill_between(x_axis_list, lower_bound_y, upper_bound_y, alpha=0.2)      # this is the shaded region\n",
    "plt.plot(x_axis_list, y_axis_list)      # this is the line itself\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f68f2d1",
   "metadata": {},
   "source": [
    "### Bar Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119289f",
   "metadata": {},
   "source": [
    "#### Using `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abe37b",
   "metadata": {},
   "source": [
    "```py\n",
    "# Bar Chart\n",
    "plt.bar(x_axis_list, y_axis_list)\n",
    "\n",
    "# Side by side bar chart\n",
    "[t*element + w*n for element in range(d)]\n",
    "    # China Data (blue bars)\n",
    "    n = 1  # This is our first dataset (out of 2)\n",
    "    t = 2 # Number of datasets\n",
    "    d = 7 # Number of sets of bars\n",
    "    w = 0.8 # Width of each bar\n",
    "    x_values1 = [t*element + w*n for element in range(d)]\n",
    "    # China Data (blue bars)\n",
    "    n = 1  # This is our first dataset (out of 2)\n",
    "    t = 2 # Number of datasets\n",
    "    d = 7 # Number of sets of bars\n",
    "    w = 0.8 # Width of each bar\n",
    "    x_values1 = [t*element + w*n for element in range(d)]\n",
    "\n",
    "# Stacked bar chart\n",
    "plt.bar(x_axis_list, y_axis_list1)\n",
    "plt.bar(x_axis_list, y_axis_list2, bottom=y_axis_list1)\n",
    "\n",
    "# Error bars\n",
    "plt.bar(x_axis_list, y_axis_list, yerr=error_value, capsize=10)     # yerr can be a single value (used on all bars) or a string of values for each bar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddf22f1",
   "metadata": {},
   "source": [
    "#### Using `Seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af469fd",
   "metadata": {},
   "source": [
    "```py\n",
    "# Using .countplot()\n",
    "sns.countplot(dataset)\n",
    "sns.countplot(x = 'column', data = df)\n",
    "\n",
    "# Keywords\n",
    "order=df[\"victory_status\"].value_counts(ascending=True).index       # for nominal data\n",
    "order=[\"First Year\", \"Second Year\", \"Third Year\", \"Fourth Year\"])   # for ordinal data\n",
    "\n",
    "# Using .barplot()\n",
    "sns.barplot(x_axis_list, y_axis_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017010a2",
   "metadata": {},
   "source": [
    "### Pie Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cac094",
   "metadata": {},
   "source": [
    "#### Using `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c080c656",
   "metadata": {},
   "source": [
    "```py\n",
    "plt.pie(list_values)\n",
    "plt.axis('equal')\n",
    "plt.legend(list_names)\n",
    "\n",
    "# Keywords\n",
    "labels=list_names\n",
    "autopct='%0.1f%%'       # set the \"percentage\" formatting\n",
    "                        # '%0.2f' — 2 decimal places, like 4.08\n",
    "                        # '%0.2f%%' — 2 decimal places, but with a percent sign at the end, like 4.08%. You need two consecutive percent signs because the first one acts as an escape character, so that the second one gets displayed on the chart.\n",
    "                        # '%d%%' — rounded to the nearest int and with a percent sign at the end, like 4%.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d1c77",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f8819",
   "metadata": {},
   "source": [
    "#### Using `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd5362",
   "metadata": {},
   "source": [
    "```py\n",
    "plt.hist(dataset)       # Creates histogram with 10 bins by default\n",
    "plt.hist(dataset, range=(min, max), bins=num_bins)      # limits the histogram to a certain range of the dataset and show a certain num of bins\n",
    "\n",
    "# Overlapping Histograms\n",
    "plt.hist(df, color = 'blue', label = 'category1', density = True, alpha = 0.5)\n",
    "plt.hist(df, color = 'red', label = 'category2', density = True, alpha = 0.5)\n",
    "\n",
    "# Keywords\n",
    "histtype='step'     # use this to generate an outline of the histogram\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d4b2b",
   "metadata": {},
   "source": [
    "#### Using `Seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a4f01",
   "metadata": {},
   "source": [
    "```py\n",
    "sns.histplot(x = 'column', data = df)                   # Histogram\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abbb349",
   "metadata": {},
   "source": [
    "### Plot Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc08b25",
   "metadata": {},
   "source": [
    "#### Using `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71499d",
   "metadata": {},
   "source": [
    "```py\n",
    "# Axis and labels\n",
    "plt.axis([x_min, x_max, y_min, y_max])      # Change the scale of the axes\n",
    "plt.xlabel('x_axis_label')          # Label the x axis\n",
    "plt.ylabel('y_axis_label')          # Label the y axis\n",
    "plt.title('plot_title')             # Set the title of the plot\n",
    "\n",
    "# Subplots\n",
    "plt.subplot(num_rows, num_cols, index)      # Any plt.plot() which comes after plt.subplot() will create a line plot in the specified subplot\n",
    "    # First Subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, y, color='green')\n",
    "    plt.title('First Subplot')\n",
    "\n",
    "    # Second Subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, y, color='steelblue')\n",
    "    plt.title('Second Subplot')\n",
    "\n",
    "    # Display both subplots\n",
    "    plt.show()\n",
    "plt.subplots_adjust(left=0, right=0, bottom=0, top=0, wspace=0, hspace=0)\n",
    "\n",
    "# Legends\n",
    "plt.legend(['plot 1', 'plot 2'], loc=0)     # Keyword 'loc' positions the legend on the figure. 0 = 'best'\n",
    "\n",
    "# Axes\n",
    "ax = plt.subplot()\n",
    "ax.set_xticks([1, 2, 4])\n",
    "ax.set_yticks([0.1, 0.6, 0.8])\n",
    "ax.set_yticklabels(['10%', '60%', '80%'], rotation=30)      # use the rotation keyword to rotate labels by a number of debrees\n",
    "\n",
    "# Figures\n",
    "plt.figure(figsize=(4, 10))                 # Set the size of the figure\n",
    "plt.plot(x, parabola)\n",
    "plt.savefig('tall_and_narrow.png')          # Save the figure to a specific format\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d986f",
   "metadata": {},
   "source": [
    "#### Using `Seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026780b1",
   "metadata": {},
   "source": [
    "```py\n",
    "sns.boxplot(x='column', data=df)                        # Single Boxplot\n",
    "sns.boxplot(x = 'category', y = 'values', data = df)    # Side by Side Boxplots\n",
    "sns.scatterplot(x='x', y='y', data=df)                  # Scatter plot\n",
    "sns.heatmap(df.corr(), annot=True)                      # Correlation heatmap\n",
    "\n",
    "df.column.value_counts().plot.pie()                     # Pie chart\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d8766",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5d261",
   "metadata": {},
   "source": [
    "```py\n",
    "model = sm.OLS.from_formula('weight ` height', data = body_measurements)        # Ordinary Least Squares (OLS) function\n",
    "results = model.fit()                                   # \n",
    "print(results.summary())\n",
    "print(results.params)\n",
    "\n",
    "newdata = {'height':[160]}                              # make a new set of data\n",
    "print(results.predict(newdata))                         # Use the model to make a prediction given a new set of data\n",
    "\n",
    "fitted_values = results.predict(body_measurements)      # We can calculate the fitted values using .predict() by passing in the original data.\n",
    "residuals = body_measurements.weight - fitted_values    # Residuals are the differences between each fitted value and the true value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00877364",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17bd01a",
   "metadata": {},
   "source": [
    "```py\n",
    "df.column.mean()                    # Mean\n",
    "df.column.median()                  # Median\n",
    "df.column.mode()                    # Mode\n",
    "trim_mean(df.column, proportiontocut=0.1)   # Trimmed mean\n",
    "df.column.max() - df.column.min()   # Range\n",
    "iqr(df['column'])                   # Interquartile range\n",
    "df.column.var()                     # Variance\n",
    "df.column.std()                     # Standard Deviation\n",
    "df.column.mad()                     # Mean Absolute Deviation\n",
    "np.cov(df.column1, df.column2)      # Covariance\n",
    "var, p = pearsonr(df.column1, df.column2)   # Pearson correlation\n",
    "chi2, pval, dof, expected = chi2_contingency(contingency_table)     # The Chi-Square Statistic\n",
    "chi2 = chi2_contingency(contingency_table)[0]\n",
    "pval = chi2_contingency(contingency_table)[1]\n",
    "dof = chi2_contingency(contingency_table)[2]\n",
    "expected = chi2_contingency(contingency_table)[3]\n",
    "\n",
    "random = np.random.choice(given_values, size = size, replace = True/False)     # Numpy function to generate a random value\n",
    "\n",
    "stats.binom.pmf(x, n, p)            # Calculate the PMF of the binomial distribution of any value | x = the value of interest | n = the num of trials | p = the probability of success\n",
    "stats.binom.cdf(x, n, p)            # Calculate the CDF of the binomial distribution of any value | x = the value of interest (probability of this value or less) | n = the sample size | p = the probability of success\n",
    "stats.norm.cdf(x, loc, scale)       # Calculate the CDF of the normal distribution | x = the value of interest | loc = the mean of the probability distribution | scale = the standard deviation of the probably distribution\n",
    "stats.poisson.pmf(p, lambda)        # Calculate the PMF of the Poisson distribution | p = the probability of observing the expected value | lambda = the expected value\n",
    "stats.poisson.cdf(p, lambda)        # Calculate the CDF of the Poisson distribution | p = the probability of observing the expected value | lambda = the expected value\n",
    "stats.poisson.rvs(lambda, size = num_values)    # Generate random variants from the Poisson distribution\n",
    "np.var(distribution)                            # Calculate the variance of a sample using numpy\n",
    "min(distribution)\n",
    "max(distribution)\n",
    "\n",
    "tstat, pval = ttest_1samp(sample_distribution, expected_mean)       # Implementing a one-sample T-Test\n",
    "p_value = binom_test(num_observed_successes, n=num_of_trials, p=expected_probability_of_success)    # Two-sided (2-sided) Binomial Testing with SciPy\n",
    "                                                                                                    # use `alternative='less'` to run a 1-sided test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e880aa2",
   "metadata": {},
   "source": [
    "### Central Limit Theorem (CLT)\n",
    "The `population` object is a list containing all wages in the full population.\n",
    "\n",
    "In each iteration of the loop, we do the following:\n",
    "- take a random sample of 150 wages from the population\n",
    "- store the sample mean in a list called `sample_means`\n",
    "\n",
    "After collecting 10k sample means, we inspect them using a histogram.\n",
    "```py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "sample_means = []\n",
    "\n",
    "for i in range(10000):                          # the for loop whic iterates 10k times to collect 10k sample means\n",
    "    samp = random.sample(population, 150)       # using the `.sample()` method of random to get a sample from `population` which has 150 individual wages\n",
    "    sample_means.append(np.mean(samp))          # append each sample population to the `sample_means` list object\n",
    "\n",
    "plt.hist(sample_means, bins = 30)\n",
    "plt.vlines(np.mean(sample_means), 0, 1000, lw=3, linestyles='dashed')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1bd111",
   "metadata": {},
   "source": [
    "While a researcher or data scientist probably does not know the population standard deviation, they can use the standard deviation of their sample to estimate it.\n",
    "\n",
    "Let’s return to the data scientist who collected a single sample of 150 wages and calculated an average wage of 17.74 dollars. To quantify the uncertainty around this sample mean, this data scientist can first estimate the standard error:\n",
    "```py\n",
    "std_error = np.std(my_sample)/(150 ** 0.5)      # the standard deviation (STD) of the sample mean is calculated as the population deviation divided by the square root of the sample size\n",
    "                                                # n ** 0.5 is an expression for square root\n",
    "print(std_error)\n",
    "# output: 1.275\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a23b9",
   "metadata": {},
   "source": [
    "Then, leveraging the part of the CLT that says the sampling distribution is normally distributed, our data scientist can use a nifty property of normal distributions: 95% of normally distributed values are within about 1.96 standard deviations of the mean. This allows the data scientist to estimate the width of the sampling distribution above, without actually knowing the population distribution!\n",
    "\n",
    "First, the data scientist needs to multiply 1.96 by the estimated standard error: 1.96 * 1.275 = 2.50. The interpretation of this number is as follows:\n",
    "- Imagine taking a large number of samples of size 150 from a population with the same amount of variation as in the observed sample.\n",
    "- 95% of those samples would be within about 2.50 dollars from the true population mean.\n",
    "- Therefore, there is about a 95% probability that the observed sample mean of 17.74 is no more than 2.50 dollars away from the population mean. In other words, there is about a 95% probability that the population mean is between 15.24 and 20.24. This is referred to as a 95% confidence interval.\n",
    "\n",
    "Note that the estimate of 2.50 is similar to the value of 2.87 that we calculated as all-knowing beings with access to the full population. With only a single sample in hand, the data scientist can express the uncertainty in their sample mean fairly accurately!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776de40a",
   "metadata": {},
   "source": [
    "### Writing a Binomial Test Function\n",
    "```py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import binom_test\n",
    "\n",
    "def simulation_binomial_test(observed_successes, n, p):\n",
    "  #initialize null_outcomes\n",
    "  null_outcomes = []\n",
    "  \n",
    "  #generate the simulated null distribution\n",
    "  for i in range(10000):\n",
    "    simulated_monthly_visitors = np.random.choice(['y', 'n'], size=n, p=[p, 1-p])\n",
    "    num_purchased = np.sum(simulated_monthly_visitors == 'y')\n",
    "    null_outcomes.append(num_purchased)\n",
    "\n",
    "  #calculate a 1-sided p-value\n",
    "  null_outcomes = np.array(null_outcomes)\n",
    "  p_value = np.sum(null_outcomes <= observed_successes)/len(null_outcomes) \n",
    "  \n",
    "  #return the p-value\n",
    "  return p_value\n",
    "\n",
    "#Test your function below by uncommenting the code below. You should see that your simulation function gives you a very similar answer to the binom_test function from scipy:\n",
    "\n",
    "p_value1 = simulation_binomial_test(45, 500, .1)\n",
    "print(\"simulation p-value: \", p_value1)\n",
    "\n",
    "p_value2 = binom_test(45, 500, .1, alternative = 'less')\n",
    "print(\"binom_test p-value: \", p_value2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989d9dd",
   "metadata": {},
   "source": [
    "## Machine Learning Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46586de3",
   "metadata": {},
   "source": [
    "```py\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c79a2",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8912f",
   "metadata": {},
   "source": [
    "```py\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['encoded'] = encoder.fit_transform(df['categorical_column'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a633f7",
   "metadata": {},
   "source": [
    "# Python Data Analysis Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca3ecf",
   "metadata": {},
   "source": [
    "## Common Libraries to Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aed245",
   "metadata": {},
   "source": [
    "```py\n",
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "from pandas.plotting import lag_plot, autocorrelation_plot\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import iqr, ttest_ind, pearsonr, trim_mean, chi2_contingency, ttest_1samp, binom_test\n",
    "\n",
    "# Machine Learning & Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f873358",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8da156",
   "metadata": {},
   "source": [
    "```py\n",
    "df.dropna()                         # Drop all rows with missing values\n",
    "df.dropna(subset=['column'])        # Drop only rows with missing values in the specified column\n",
    "df.fillna(value)                    # Fill missing values\n",
    "df.duplicated().sum()               # Count duplicates\n",
    "df.drop_duplicates(inplace=True, subset=['item'])   # Remove duplicate rows\n",
    "                                                    # If we wanted to remove every row with a duplicate value in the item column, we could specify a subset\n",
    "                                                    \n",
    "# map() applies the str.lower() function to each of the columns in our dataset to convert the column names to all lowercase\n",
    "df.columns = map(str.lower, df.columns)\n",
    "# `axis=1` refers to the columns, `axis=0` would refer to the rows\n",
    "# In the dictionary the key refers to the original column name and the value refers to the new column name {'oldname1': 'newname1', 'oldname2': 'newname2'}\n",
    "df = df.rename({'key1':'value1', 'key2':'value2'}, axis=1)\n",
    "# Replace column values by overwriting the DF with a DF filtered to only certain values\n",
    "df.column = df.column.where(df.column expression True)\n",
    "# Remove prefixes from certain values\n",
    "df.column = df.column.str.lstrip('string_to_strip')     # use lstrip() to left strip (remove prefixes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd897f35",
   "metadata": {},
   "source": [
    "### Helpful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabe820f",
   "metadata": {},
   "source": [
    "`.str` | `.str.split()` | `.str.get()`\n",
    "\n",
    "Example: Let’s say we have a column called “type” with data entries in the format \"admin_US\" or \"user_Kenya\".\n",
    "\n",
    "```py\n",
    "# Split the string and save it as `string_split`\n",
    "string_split = df['type'].str.split('_')\n",
    " \n",
    "# Create the 'usertype' column\n",
    "df['usertype'] = string_split.str.get(0)\n",
    " \n",
    "# Create the 'country' column\n",
    "df['country'] = string_split.str.get(1)\n",
    "```\n",
    "<br>\n",
    "<br>\n",
    "Example: It would be helpful to separate out data like \"30 lunges\" into 2 columns with the number of reps, \"30\", and the type of exercise, \"lunges\".\n",
    "\n",
    "```py\n",
    "split_df = df['exerciseDescription'].str.split('(\\d+)', expand=True)    # To extract the numbers from the string we can use pandas’ .str.split() function.\n",
    "df.reps = pd.to_numeric(split_df[1])                                    # Then, we can assign columns from this DataFrame to the original df.\n",
    "df.exercise = split_df[0].replace('[\\- ]', '', regex=True)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491df48",
   "metadata": {},
   "source": [
    "`regex`\n",
    "- For OR, use a pipe character `|`\n",
    "- Character sets: match any character in the brackets. Each set of brackets only gives a match for a single character. `[]`\n",
    "- Negate operator, use a caret `^`\n",
    "- Wildcard operator, use a period `.`\n",
    "- Escape character, use a backslash `\\`\n",
    "- Ranges `[ - ]`\n",
    "- Shorthand\n",
    "    - `\\w` = `[A-Za-z0-9]`\n",
    "    - `\\d` = `[0-9]`\n",
    "    - `\\s` = `[\\t\\r\\n\\f\\v]`\n",
    "    - `\\W` = `[^A-Za-z0-9]`\n",
    "    - `\\D` = `[^0-9]`\n",
    "    - `\\S` = `[^\\t\\r\\n\\f\\v]`\n",
    "- Grouping, use parentheses `()`, like `(this | that)`\n",
    "- Quantifiers, match one or more of the values in the curly braces `{4,7}`\n",
    "- Optional Quantifier, makes the preceeding character optional `?`\n",
    "- Quantifier, 1 or more `+`\n",
    "- Quantifier, 0, 1, or more `*`\n",
    "- Anchors, mark the beginning and ending characters `^string$`\n",
    "<br>\n",
    "<br>\n",
    "```py\n",
    "fruit.price = fruit['price'].replace('[\\$,]', '', regex=True)       # Use a regex to get rid of dollar signs in a `price` variable\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0b453",
   "metadata": {},
   "source": [
    "`.to_numeric()`\n",
    "\n",
    "```py\n",
    "pd.to_numeric(column)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d613fcf",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e848b2e6",
   "metadata": {},
   "source": [
    "```py\n",
    "# Read file\n",
    "df = read_csv('file.csv')   # Read csv data into a pandas DataFrame\n",
    "# Read multiple files\n",
    "filenames = glob.glob('file*.csv')                      # Finds any file that starts with `file` and has an extension of `.csv`. \n",
    "file_list = []\n",
    "file_list = [pd.read_csv(file) for file in filenames]   # Opens each file, reads into a DataFrame\n",
    "df = pd.concat(file_list)                               # Concatenates those DataFrames together.\n",
    "\n",
    "df.head()               # First 5 rows\n",
    "df.info()               # Data types & non-null counts\n",
    "df.describe()           # Summary statistics\n",
    "df.duplicated()         # Returns a boolean value on each row if the whole row is an exact match with another whole row\n",
    "df.columns              # Column names\n",
    "df.shape                # Rows and columns\n",
    "df.dtypes               # Data types for each column\n",
    "df.nunique()            # .nunique() counts the number of unique values in each column\n",
    "# Count the number of missing values in each column\n",
    "df.isnull().sum()       # Missing values\n",
    "df.isna().sum()\n",
    "df.column.value_counts()       # List the unique values in a column and the count of each value\n",
    "df.column.value_counts(normalize=True)  # List the unqiue values in a column and the proportion of each value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb1cd92",
   "metadata": {},
   "source": [
    "### Melt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f74d861",
   "metadata": {},
   "source": [
    "```py\n",
    "df = df.melt(\n",
    "    frame=df, \n",
    "    id_vars=\"Index Column\",             # the column(s) of the old DataFrame to preserve\n",
    "    value_vars=['Column Values'],       # the column(s) of the old DataFrame that you want to turn into variables\n",
    "    value_name='Value Column Name',     # what to call the column of the new DataFrame that stores the values\n",
    "    var_name='Variable Column Name'     # what to call the column of the new DataFrame that stores the variables\n",
    "    )           \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ec026",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "```py\n",
    "students = students.melt(\n",
    "    id_vars=['full_name', 'gender_age', 'grade'], \n",
    "    value_vars=['fractions', 'probability'], \n",
    "    value_name='score', \n",
    "    var_name='exam')\n",
    "```\n",
    "The `id_vars`, the columns that should stay the same in the new DataFrame, are `full_name`, `gender_age`, and `grade`.\n",
    "\n",
    "We want to turn `fractions` and `probability` into variables, so those are the `value_vars`.\n",
    "\n",
    "We want to call that new column `exam`, so that’s our `var_name`.\n",
    "\n",
    "The values previously held in `fractions` and `probability`, we want to store in a column called `score`, so that’s the `value_name`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ff6ae",
   "metadata": {},
   "source": [
    "### Cross Tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce791f9",
   "metadata": {},
   "source": [
    "```py\n",
    "pd.crosstab(df.column1, df.column2)     # Crosstab function from pandas to create a contingency table\n",
    "crosstab_object / len(df)               # Converting the crosstab object to proportional values\n",
    "crosstab_prop_object.sum(axis = 0)      # Get marginal proportion of the Y axis\n",
    "crosstab_prop_object.sum(axis = 1)      # Get marginal proprotion of the X axis\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc50030",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "```py\n",
    "pd.crosstab(\n",
    "    # tabulates the boroughs as the index\n",
    "    restaurants['boro'],  \n",
    "    # tabulates the number of missing values in the url column as columns\n",
    "    restaurants['url'].isna(), \n",
    "    # names the rows\n",
    "    rownames = ['boro'],\n",
    "    # names the columns \n",
    "    colnames = ['url is na']) \n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "url is na   False   True\n",
    "boro        \n",
    "Bronx         1   1\n",
    "Brooklyn       2      4\n",
    "Manhattan     11      2\n",
    "Queens       2    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094c91b",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be40d02",
   "metadata": {},
   "source": [
    "### Lag Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186620a",
   "metadata": {},
   "source": [
    "```py\n",
    "# lag scatter plot\n",
    "lag_plot(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed3ef20",
   "metadata": {},
   "source": [
    "How can we interpret a lag scatter plot?\n",
    "- If the points move from the bottom left to the top right, this indicates a positive correlation between observations and their lag 1 values. For example, high sales on one day are associated with high sales on the previous day.\n",
    "- If the points move from the top left to the bottom right, this indicates a negative correlation between observations and their lag 1 values. For example, high sales on one day are associated with low sales on the previous day and vice versa.\n",
    "- If there is no identifiable structure in the lag plot, this indicates the data is random, and there is no association between values at consecutive time points. For example, sales on one day tell you no information about expected sales on the following day.\n",
    "\n",
    "Exploring the relationship between an observation and a lag of that observation is useful for helping us determine whether a dataset is random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843018aa",
   "metadata": {},
   "source": [
    "### Autocorrelation Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd091c4",
   "metadata": {},
   "source": [
    "```py\n",
    "# autocorrelation plot\n",
    "autocorrelation_plot(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de6438",
   "metadata": {},
   "source": [
    "How can we interpret a lag scatter plot?\n",
    "- If the points move from the bottom left to the top right, this indicates a positive correlation between observations and their lag 1 values. For example, high sales on one day are associated with high sales on the previous day.\n",
    "- If the points move from the top left to the bottom right, this indicates a negative correlation between observations and their lag 1 values. For example, high sales on one day are associated with low sales on the previous day and vice versa.\n",
    "- If there is no identifiable structure in the lag plot, this indicates the data is random, and there is no association between values at consecutive time points. For example, sales on one day tell you no information about expected sales on the following day.\n",
    "\n",
    "Exploring the relationship between an observation and a lag of that observation is useful for helping us determine whether a dataset is random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8360f0",
   "metadata": {},
   "source": [
    "### Heat Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3176b1",
   "metadata": {},
   "source": [
    "#### Using `Seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea593c2",
   "metadata": {},
   "source": [
    "```py\n",
    "# Define the colormap which maps the data values to the color space defined with the diverging_palette method  \n",
    "colors = sns.diverging_palette(150, 275, s=80, l=55, n=9, as_cmap=True)\n",
    "\n",
    "# Create heatmap using the .corr method on df, set colormap to cmap\n",
    "sns.heatmap(rentals.corr(), center=0, cmap=colors, robust=True)\n",
    "\n",
    "sns.heatmap(df.corr(), annot=True)          # Correlation heatmap\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45e2f0",
   "metadata": {},
   "source": [
    "##### `Example:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb94303",
   "metadata": {},
   "source": [
    "```py\n",
    "# calculate total sales for each month\n",
    "sales = sales_data.groupby([\"year\", \"month\"]).sum()\n",
    "\n",
    "# re-format the data for the heat-map\n",
    "sales_month_year = sales.reset_index().pivot(index=\"year\", columns=\"month\", values=\"sales\")\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(sales_month_year, cbar_kws={\"label\": \"Total Sales\"})\n",
    "plt.title(\"Sales Over Time\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Year\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff5eb5",
   "metadata": {},
   "source": [
    "### Scatter Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cea748",
   "metadata": {},
   "source": [
    "#### Using `Seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4cd60a",
   "metadata": {},
   "source": [
    "```py\n",
    "sns.scatterplot(x='x', y='y', data=df)                  # Scatter plot\n",
    "sns.scatterplot(df.column1, df.column2)                 # Scatter plot\n",
    "sns.scatterplot(x=df.x, y=df.y, data=df, hue=df.z)      # Multivariate scatter plot\n",
    "sns.scatterplot(x=df.1, y=df.2, hue=df.3, style=df., data=df)   # 4 variable scatter plot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6543d2fd",
   "metadata": {},
   "source": [
    "#### Using `Plotly`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4c54f",
   "metadata": {},
   "source": [
    "```py\n",
    "fig = px.scatter_3d(df, x=df.1, y=df.2, z=df.3, color=df.4)             # 3 dimensional scatter plot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0b837",
   "metadata": {},
   "source": [
    "### Line Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25b5d1",
   "metadata": {},
   "source": [
    "#### Using `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeee35b",
   "metadata": {},
   "source": [
    "```py\n",
    "# Single line plot\n",
    "plt.plot(x_axis_list, y_axis_list)\n",
    "plt.show()\n",
    "\n",
    "# Multi-line plot. Matplotlib will automatically place the two lines on the same axes and give them different colors if you call plt.plot() twice.\n",
    "plt.plot(x_axis_list, y_axis_list)\n",
    "plt.plot(x_axis_list, y_axis_list_2)\n",
    "plt.show()\n",
    "\n",
    "# Linestyles keywords\n",
    "color = 'green' or '#AAAAAA'      # We can specify a different color for a line by using the keyword color with either an HTML color name or a Hex code\n",
    "linestyle = '--' or ':' or ''       # Change the linestyle to dotted or dashed\n",
    "marker = 'o' or 's' or '*'          # Add a line marker like a dot or a square\n",
    "\n",
    "# Shaded error region\n",
    "lower_bound_y = [i - 2 for i in y_axis_list]    # represents an error of 2 for each y value\n",
    "upper_bound_y = [i + 2 for i in y_axis_list]\n",
    "plt.fill_between(x_axis_list, lower_bound_y, upper_bound_y, alpha=0.2)      # this is the shaded region\n",
    "plt.plot(x_axis_list, y_axis_list)      # this is the line itself\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f68f2d1",
   "metadata": {},
   "source": [
    "### Bar Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119289f",
   "metadata": {},
   "source": [
    "#### Using `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abe37b",
   "metadata": {},
   "source": [
    "```py\n",
    "# Bar Chart\n",
    "plt.bar(x_axis_list, y_axis_list)\n",
    "\n",
    "# Side by side bar chart\n",
    "[t*element + w*n for element in range(d)]\n",
    "    # China Data (blue bars)\n",
    "    n = 1  # This is our first dataset (out of 2)\n",
    "    t = 2 # Number of datasets\n",
    "    d = 7 # Number of sets of bars\n",
    "    w = 0.8 # Width of each bar\n",
    "    x_values1 = [t*element + w*n for element in range(d)]\n",
    "    # China Data (blue bars)\n",
    "    n = 1  # This is our first dataset (out of 2)\n",
    "    t = 2 # Number of datasets\n",
    "    d = 7 # Number of sets of bars\n",
    "    w = 0.8 # Width of each bar\n",
    "    x_values1 = [t*element + w*n for element in range(d)]\n",
    "\n",
    "# Stacked bar chart\n",
    "plt.bar(x_axis_list, y_axis_list1)\n",
    "plt.bar(x_axis_list, y_axis_list2, bottom=y_axis_list1)\n",
    "\n",
    "# Error bars\n",
    "plt.bar(x_axis_list, y_axis_list, yerr=error_value, capsize=10)     # yerr can be a single value (used on all bars) or a string of values for each bar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddf22f1",
   "metadata": {},
   "source": [
    "#### Using `Seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af469fd",
   "metadata": {},
   "source": [
    "```py\n",
    "# Using .countplot()\n",
    "sns.countplot(dataset)\n",
    "sns.countplot(x = 'column', data = df)\n",
    "sns.countplot(x='category', hue='category2', data=df)               # Side by side bar plot\n",
    "\n",
    "# Keywords\n",
    "order=df[\"victory_status\"].value_counts(ascending=True).index       # for nominal data\n",
    "order=[\"First Year\", \"Second Year\", \"Third Year\", \"Fourth Year\"])   # for ordinal data\n",
    "\n",
    "# Using .barplot()\n",
    "sns.barplot(x_axis_list, y_axis_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017010a2",
   "metadata": {},
   "source": [
    "### Pie Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cac094",
   "metadata": {},
   "source": [
    "#### Using `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c080c656",
   "metadata": {},
   "source": [
    "```py\n",
    "plt.pie(list_values)\n",
    "plt.axis('equal')\n",
    "plt.legend(list_names)\n",
    "\n",
    "# Keywords\n",
    "labels=list_names\n",
    "autopct='%0.1f%%'       # set the \"percentage\" formatting\n",
    "                        # '%0.2f' — 2 decimal places, like 4.08\n",
    "                        # '%0.2f%%' — 2 decimal places, but with a percent sign at the end, like 4.08%. You need two consecutive percent signs because the first one acts as an escape character, so that the second one gets displayed on the chart.\n",
    "                        # '%d%%' — rounded to the nearest int and with a percent sign at the end, like 4%.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d1c77",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f8819",
   "metadata": {},
   "source": [
    "#### Using `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd5362",
   "metadata": {},
   "source": [
    "```py\n",
    "plt.hist(dataset)       # Creates histogram with 10 bins by default\n",
    "plt.hist(dataset, range=(min, max), bins=num_bins)      # limits the histogram to a certain range of the dataset and show a certain num of bins\n",
    "\n",
    "# Overlapping Histograms\n",
    "plt.hist(df, color = 'blue', label = 'category1', density = True, alpha = 0.5)\n",
    "plt.hist(df, color = 'red', label = 'category2', density = True, alpha = 0.5)\n",
    "\n",
    "# Keywords\n",
    "histtype='step'     # use this to generate an outline of the histogram\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d4b2b",
   "metadata": {},
   "source": [
    "#### Using `Seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a4f01",
   "metadata": {},
   "source": [
    "```py\n",
    "sns.histplot(x = 'column', data = df)           # Histogram\n",
    "sns.displot(dataset, bins=10, kde=False)        # Histogram\n",
    "        kind='hist' or 'kde' or 'ecdf'          # Histogram type is default | KDE = Kernel Density Estimates | ECDF = Emprical Cumulative Distribution Functions\n",
    "```\n",
    "Use histplot() when you need precise control over a single histogram on a specific Axes object, or when building complex figures where you manage the Axes objects manually.\n",
    "\n",
    "Use displot() when you want to quickly visualize distributions of data, especially across different categories, and benefit from the convenience of Seaborn's figure-level faceting capabilities for creating multiple subplots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc15fbb",
   "metadata": {},
   "source": [
    "### Box Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f8681",
   "metadata": {},
   "source": [
    "#### Using `Seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff830b6",
   "metadata": {},
   "source": [
    "```py\n",
    "sns.boxplot(x='column', data=df)                        # Single Boxplot\n",
    "sns.boxplot(x = 'category', y = 'values', data = df)    # Side by Side Boxplots\n",
    "sns.boxplot(x=df.1, y=df.2, hue=df.3, data=df)          # Side by side GROUPED box plots\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abbb349",
   "metadata": {},
   "source": [
    "### Plot Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc08b25",
   "metadata": {},
   "source": [
    "#### Using `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71499d",
   "metadata": {},
   "source": [
    "```py\n",
    "# Axis and labels\n",
    "plt.axis([x_min, x_max, y_min, y_max])      # Change the scale of the axes\n",
    "plt.xlabel('x_axis_label')          # Label the x axis\n",
    "plt.ylabel('y_axis_label')          # Label the y axis\n",
    "plt.title('plot_title')             # Set the title of the plot\n",
    "plt.xticks(rotation=90, fontsize=10)        # Rotate the x axis labels by 90 degrees and set fontsize = 10\n",
    "\n",
    "# Subplots\n",
    "plt.subplot(num_rows, num_cols, index)      # Any plt.plot() which comes after plt.subplot() will create a line plot in the specified subplot\n",
    "    # First Subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, y, color='green')\n",
    "    plt.title('First Subplot')\n",
    "\n",
    "    # Second Subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, y, color='steelblue')\n",
    "    plt.title('Second Subplot')\n",
    "\n",
    "    # Display both subplots\n",
    "    plt.show()\n",
    "plt.subplots_adjust(left=0, right=0, bottom=0, top=0, wspace=0, hspace=0)\n",
    "\n",
    "# Legends\n",
    "plt.legend(['plot 1', 'plot 2'], loc=0)     # Keyword 'loc' positions the legend on the figure. 0 = 'best'\n",
    "\n",
    "# Axes\n",
    "ax = plt.subplot()\n",
    "ax.set_xticks([1, 2, 4])\n",
    "ax.set_yticks([0.1, 0.6, 0.8])\n",
    "ax.set_yticklabels(['10%', '60%', '80%'], rotation=30)      # use the rotation keyword to rotate labels by a number of degrees\n",
    "ax.bar_label(ax.containers[0])              # Display the bar values on the bars themselves\n",
    "\n",
    "# Figures\n",
    "plt.figure(figsize=(4, 10))                 # Set the size of the figure\n",
    "plt.plot(x, parabola)\n",
    "plt.savefig('tall_and_narrow.png')          # Save the figure to a specific format\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d986f",
   "metadata": {},
   "source": [
    "#### Using `Seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026780b1",
   "metadata": {},
   "source": [
    "```py\n",
    "df.column.value_counts().plot.pie()                     # Pie chart\n",
    "\n",
    "# Keywords\n",
    "palette='bright'          # Set the color scheme for the plot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d8766",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5d261",
   "metadata": {},
   "source": [
    "```py\n",
    "model = sm.OLS.from_formula('weight ` height', data = body_measurements)        # Ordinary Least Squares (OLS) function\n",
    "results = model.fit()                                   # \n",
    "print(results.summary())\n",
    "print(results.params)\n",
    "\n",
    "newdata = {'height':[160]}                              # make a new set of data\n",
    "print(results.predict(newdata))                         # Use the model to make a prediction given a new set of data\n",
    "\n",
    "fitted_values = results.predict(body_measurements)      # We can calculate the fitted values using .predict() by passing in the original data.\n",
    "residuals = body_measurements.weight - fitted_values    # Residuals are the differences between each fitted value and the true value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00877364",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17bd01a",
   "metadata": {},
   "source": [
    "```py\n",
    "df.column.mean()                    # Mean\n",
    "df.column.median()                  # Median\n",
    "df.column.mode()                    # Mode\n",
    "trim_mean(df.column, proportiontocut=0.1)   # Trimmed mean\n",
    "df.column.max() - df.column.min()   # Range\n",
    "iqr(df['column'])                   # Interquartile range\n",
    "df.column.var()                     # Variance\n",
    "df.column.std()                     # Standard Deviation\n",
    "df.column.mad()                     # Mean Absolute Deviation\n",
    "np.cov(df.column1, df.column2)      # Covariance\n",
    "var, p = pearsonr(df.column1, df.column2)   # Pearson correlation\n",
    "chi2, pval, dof, expected = chi2_contingency(contingency_table)     # The Chi-Square Statistic\n",
    "chi2 = chi2_contingency(contingency_table)[0]\n",
    "pval = chi2_contingency(contingency_table)[1]\n",
    "dof = chi2_contingency(contingency_table)[2]\n",
    "expected = chi2_contingency(contingency_table)[3]\n",
    "\n",
    "random = np.random.choice(given_values, size = size, replace = True/False)     # Numpy function to generate a random value\n",
    "\n",
    "stats.binom.pmf(x, n, p)            # Calculate the PMF of the binomial distribution of any value | x = the value of interest | n = the num of trials | p = the probability of success\n",
    "stats.binom.cdf(x, n, p)            # Calculate the CDF of the binomial distribution of any value | x = the value of interest (probability of this value or less) | n = the sample size | p = the probability of success\n",
    "stats.norm.cdf(x, loc, scale)       # Calculate the CDF of the normal distribution | x = the value of interest | loc = the mean of the probability distribution | scale = the standard deviation of the probably distribution\n",
    "stats.poisson.pmf(p, lambda)        # Calculate the PMF of the Poisson distribution | p = the probability of observing the expected value | lambda = the expected value\n",
    "stats.poisson.cdf(p, lambda)        # Calculate the CDF of the Poisson distribution | p = the probability of observing the expected value | lambda = the expected value\n",
    "stats.poisson.rvs(lambda, size = num_values)    # Generate random variants from the Poisson distribution\n",
    "np.var(distribution)                            # Calculate the variance of a sample using numpy\n",
    "min(distribution)\n",
    "max(distribution)\n",
    "\n",
    "tstat, pval = ttest_1samp(sample_distribution, expected_mean)       # Implementing a one-sample T-Test\n",
    "p_value = binom_test(num_observed_successes, n=num_of_trials, p=expected_probability_of_success)    # Two-sided (2-sided) Binomial Testing with SciPy\n",
    "                                                                                                    # use `alternative='less'` to run a 1-sided test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e880aa2",
   "metadata": {},
   "source": [
    "### Central Limit Theorem (CLT)\n",
    "The `population` object is a list containing all wages in the full population.\n",
    "\n",
    "In each iteration of the loop, we do the following:\n",
    "- take a random sample of 150 wages from the population\n",
    "- store the sample mean in a list called `sample_means`\n",
    "\n",
    "After collecting 10k sample means, we inspect them using a histogram.\n",
    "```py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "sample_means = []\n",
    "\n",
    "for i in range(10000):                          # the for loop whic iterates 10k times to collect 10k sample means\n",
    "    samp = random.sample(population, 150)       # using the `.sample()` method of random to get a sample from `population` which has 150 individual wages\n",
    "    sample_means.append(np.mean(samp))          # append each sample population to the `sample_means` list object\n",
    "\n",
    "plt.hist(sample_means, bins = 30)\n",
    "plt.vlines(np.mean(sample_means), 0, 1000, lw=3, linestyles='dashed')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1bd111",
   "metadata": {},
   "source": [
    "While a researcher or data scientist probably does not know the population standard deviation, they can use the standard deviation of their sample to estimate it.\n",
    "\n",
    "Let’s return to the data scientist who collected a single sample of 150 wages and calculated an average wage of 17.74 dollars. To quantify the uncertainty around this sample mean, this data scientist can first estimate the standard error:\n",
    "```py\n",
    "std_error = np.std(my_sample)/(150 ** 0.5)      # the standard deviation (STD) of the sample mean is calculated as the population deviation divided by the square root of the sample size\n",
    "                                                # n ** 0.5 is an expression for square root\n",
    "print(std_error)\n",
    "# output: 1.275\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a23b9",
   "metadata": {},
   "source": [
    "Then, leveraging the part of the CLT that says the sampling distribution is normally distributed, our data scientist can use a nifty property of normal distributions: 95% of normally distributed values are within about 1.96 standard deviations of the mean. This allows the data scientist to estimate the width of the sampling distribution above, without actually knowing the population distribution!\n",
    "\n",
    "First, the data scientist needs to multiply 1.96 by the estimated standard error: 1.96 * 1.275 = 2.50. The interpretation of this number is as follows:\n",
    "- Imagine taking a large number of samples of size 150 from a population with the same amount of variation as in the observed sample.\n",
    "- 95% of those samples would be within about 2.50 dollars from the true population mean.\n",
    "- Therefore, there is about a 95% probability that the observed sample mean of 17.74 is no more than 2.50 dollars away from the population mean. In other words, there is about a 95% probability that the population mean is between 15.24 and 20.24. This is referred to as a 95% confidence interval.\n",
    "\n",
    "Note that the estimate of 2.50 is similar to the value of 2.87 that we calculated as all-knowing beings with access to the full population. With only a single sample in hand, the data scientist can express the uncertainty in their sample mean fairly accurately!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776de40a",
   "metadata": {},
   "source": [
    "### Writing a Binomial Test Function\n",
    "```py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import binom_test\n",
    "\n",
    "def simulation_binomial_test(observed_successes, n, p):\n",
    "  #initialize null_outcomes\n",
    "  null_outcomes = []\n",
    "  \n",
    "  #generate the simulated null distribution\n",
    "  for i in range(10000):\n",
    "    simulated_monthly_visitors = np.random.choice(['y', 'n'], size=n, p=[p, 1-p])\n",
    "    num_purchased = np.sum(simulated_monthly_visitors == 'y')\n",
    "    null_outcomes.append(num_purchased)\n",
    "\n",
    "  #calculate a 1-sided p-value\n",
    "  null_outcomes = np.array(null_outcomes)\n",
    "  p_value = np.sum(null_outcomes <= observed_successes)/len(null_outcomes) \n",
    "  \n",
    "  #return the p-value\n",
    "  return p_value\n",
    "\n",
    "#Test your function below by uncommenting the code below. You should see that your simulation function gives you a very similar answer to the binom_test function from scipy:\n",
    "\n",
    "p_value1 = simulation_binomial_test(45, 500, .1)\n",
    "print(\"simulation p-value: \", p_value1)\n",
    "\n",
    "p_value2 = binom_test(45, 500, .1, alternative = 'less')\n",
    "print(\"binom_test p-value: \", p_value2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989d9dd",
   "metadata": {},
   "source": [
    "## Machine Learning Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46586de3",
   "metadata": {},
   "source": [
    "```py\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c79a2",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8912f",
   "metadata": {},
   "source": [
    "```py\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['encoded'] = encoder.fit_transform(df['categorical_column'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
